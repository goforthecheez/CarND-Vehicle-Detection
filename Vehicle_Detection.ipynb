{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection\n",
    "\n",
    "I build an SVM classifier to distinguish vehicles from non-vehicles.\n",
    "\n",
    "Outline:\n",
    "\n",
    "1. Collect summary statistics on data.\n",
    "2. Explore and define color features.\n",
    "3. Define Histogram of Oriented Gradient features.\n",
    "4. Extract and normalize features.\n",
    "5. Build and train an SVM classifier.\n",
    "6. Search for vehicle in an image using a sliding window search.\n",
    "7. Combine overlapping windows & eliminate false positives.\n",
    "8. Generate final video output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect summary statistics on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"Project: Vehicle Detection and Tracking, 19. Data Exploration\".\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Images are divided up into vehicles and non-vehicles\n",
    "cars = glob.glob('./vehicles/*/*.png')\n",
    "notcars = glob.glob('./non-vehicles/*/*.png')\n",
    "        \n",
    "# Define a function to return some characteristics of the dataset \n",
    "def data_look(car_list, notcar_list):\n",
    "    data_dict = {}\n",
    "    # Define a key in data_dict \"n_cars\" and store the number of car images\n",
    "    data_dict[\"n_cars\"] = len(car_list)\n",
    "    # Define a key \"n_notcars\" and store the number of notcar images\n",
    "    data_dict[\"n_notcars\"] = len(notcar_list)\n",
    "    # Read in a test image, either car or notcar\n",
    "    img = plt.imread(car_list[0])\n",
    "    # Define a key \"image_shape\" and store the test image shape 3-tuple\n",
    "    data_dict[\"image_shape\"] = img.shape\n",
    "    # Define a key \"data_type\" and store the data type of the test image.\n",
    "    data_dict[\"data_type\"] = type(img[0, 0, 0])\n",
    "    # Return data_dict\n",
    "    return data_dict\n",
    "    \n",
    "data_info = data_look(cars, notcars)\n",
    "\n",
    "print('The data has a count of', \n",
    "      data_info[\"n_cars\"], 'cars and', \n",
    "      data_info[\"n_notcars\"], 'non-cars')\n",
    "print('of size:', data_info[\"image_shape\"],\n",
    "      'and data type:', data_info[\"data_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset does not suffer from a significant class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore and define color features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"Project: Vehicle Detection and Tracking, 15. Explore Color Spaces\".\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot3d(pixels, colors_rgb, axis_labels=list(\"RGB\"), axis_limits=((0, 255), (0, 255), (0, 255))):\n",
    "    \"\"\"Plot pixels in 3D.\"\"\"\n",
    "\n",
    "    # Create figure and 3D axes\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = Axes3D(fig)\n",
    "\n",
    "    # Set axis limits\n",
    "    ax.set_xlim(*axis_limits[0])\n",
    "    ax.set_ylim(*axis_limits[1])\n",
    "    ax.set_zlim(*axis_limits[2])\n",
    "\n",
    "    # Set axis labels and sizes\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14, pad=8)\n",
    "    ax.set_xlabel(axis_labels[0], fontsize=16, labelpad=16)\n",
    "    ax.set_ylabel(axis_labels[1], fontsize=16, labelpad=16)\n",
    "    ax.set_zlabel(axis_labels[2], fontsize=16, labelpad=16)\n",
    "\n",
    "    # Plot pixel values with colors given in colors_rgb\n",
    "    ax.scatter(\n",
    "        pixels[:, :, 0].ravel(),\n",
    "        pixels[:, :, 1].ravel(),\n",
    "        pixels[:, :, 2].ravel(),\n",
    "        c=colors_rgb.reshape((-1, 3)),\n",
    "        edgecolors='none')\n",
    "\n",
    "    return ax  # return Axes3D object for further manipulation\n",
    "\n",
    "def plot_colors(img_path, img_figsize=(15, 15)):\n",
    "    \"\"\"Plots img_path and a sample of its pixels in various color spaces.\"\"\"\n",
    "    # Read a color image\n",
    "    img = cv2.imread(img_path)\n",
    "    plt.figure(figsize=img_figsize)\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Select a small fraction of pixels to plot by subsampling it\n",
    "    scale = max(img.shape[0], img.shape[1], 64) / 64  # at most 64 rows and columns\n",
    "    img_small = cv2.resize(img, (np.int(img.shape[1] / scale), np.int(img.shape[0] / scale)),\n",
    "                           interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Convert subsampled image to desired color space(s)\n",
    "    img_small_RGB = cv2.cvtColor(img_small, cv2.COLOR_BGR2RGB)  # OpenCV uses BGR, matplotlib likes RGB\n",
    "    img_small_rgb = img_small_RGB / 255.  # scaled to [0, 1], only for plotting\n",
    "    img_small_HLS = cv2.cvtColor(img_small, cv2.COLOR_BGR2HLS)\n",
    "    img_small_HSV = cv2.cvtColor(img_small, cv2.COLOR_RGB2HSV)\n",
    "    img_small_LUV = cv2.cvtColor(img_small, cv2.COLOR_BGR2Luv)\n",
    "    img_small_YUV = cv2.cvtColor(img_small, cv2.COLOR_BGR2YUV)\n",
    "    img_small_YCrCb = cv2.cvtColor(img_small, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "    # Plot and show data.\n",
    "    plot3d(img_small_RGB, img_small_rgb)\n",
    "    plt.show()\n",
    "    plot3d(img_small_HLS, img_small_rgb, axis_labels=list(\"HLS\"), axis_limits=((0, 179), (0, 255), (0, 255)))\n",
    "    plt.show()\n",
    "    plot3d(img_small_HSV, img_small_rgb, axis_labels=list(\"HSV\"), axis_limits=((0, 179), (0, 255), (0, 255)))\n",
    "    plt.show()\n",
    "    plot3d(img_small_LUV, img_small_rgb, axis_labels=list(\"LUV\"))\n",
    "    plt.show()\n",
    "    plot3d(img_small_YUV, img_small_rgb, axis_labels=list(\"YUV\"))\n",
    "    plt.show()\n",
    "    plot3d(img_small_YCrCb, img_small_rgb, axis_labels=list(\"YCrCb\"))\n",
    "    plt.show()\n",
    "           \n",
    "plot_colors('./exploration/000275.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image has black, red, and white cars. HLS color space seems to do the best job emphasizing the separation of these points from the rest of the image. HSV color space also performs well, but since HSV and HLS encode similar information in different orders, I'd like to pick one set of color features. Including both would be redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors('./exploration/000528.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image has some white (?) cars in the distance and a large black car in the lower right corner. The HLS and HSV color spaces do a good job emphasizing the black car, but HLS does a better job keeping the collection of black points in a single cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors('./exploration/001240.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image has some two nearby white cars, a distant white car, a black car, and a red car. The red car isn't very distinct in any of the color space 3D plots, but HLS color space still seems to do the best job separating out the white and black cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors('./exploration/yellow_car.png', img_figsize=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HSL color space separates the yellow points from the black points the best (diagonally), and also does a good job separating the yellow car and pale blue background points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors('./exploration/white_car.png', img_figsize=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HLS again separates the white car from the black points the best, but has the defect of treating its red backup lights as separate objects from the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors('./exploration/red_car.png', img_figsize=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this red car, HSV color space does a better job keeping the car in a single cluster than HSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors('./exploration/road.png', img_figsize=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is of a patch of road, which is pretty uniform in color and show up in all of the plots as a single cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors('./exploration/sky.png', img_figsize=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image is of a patch of sky and part of a tree. HSV and HSL color space separate out these two components. Otherwise, the points in the plots are tightly clustered, unlike in the `*_car.png` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors('./exploration/building.png', img_figsize=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HLS and HSV color space, this image has blue and red streaks similar to the `*_car.png` images, although less pronounced. A case like this might give rise to a false positive classification.\n",
    "\n",
    "Overall, HLS color space seems best suited for distinguishing cars from other objects in a scene.\n",
    "\n",
    "I next define a `bin_spatial()` function to bin the pixels in an image into `size` buckets, get the color channel information, and flatten the results into a feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"Project: Vehicle Detection and Tracking, 16. Spatial Binning of Color\".\n",
    "\n",
    "SPATIAL = 16\n",
    "\n",
    "def convert_color(img, color_space='HLS'):\n",
    "    \"\"\"Convert color from RGB color space to color_space color space.\"\"\"\n",
    "    # Apply color conversion if other than 'RGB'\n",
    "    if color_space == 'RGB':\n",
    "        feature_img = np.copy(img)\n",
    "    elif color_space == 'HLS':\n",
    "        feature_img = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    elif color_space == 'HSV':\n",
    "        feature_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    elif color_space == 'LUV':\n",
    "        feature_img = cv2.cvtColor(img, cv2.COLOR_RGB2Luv)\n",
    "    elif color_space == 'YUV':\n",
    "        feature_img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "    elif color_space == 'YCrCb':\n",
    "        feature_img = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    return feature_img\n",
    "\n",
    "# Define a function to compute color histogram features  \n",
    "# Pass the color_space flag as 3-letter all caps string\n",
    "# like 'HSV' or 'LUV' etc.\n",
    "# KEEP IN MIND IF YOU DECIDE TO USE THIS FUNCTION LATER\n",
    "# IN YOUR PROJECT THAT IF YOU READ THE IMAGE WITH \n",
    "# cv2.imread() INSTEAD YOU START WITH BGR COLOR!\n",
    "def bin_spatial(img, color_space='HLS', size=(10, 10)):\n",
    "    feature_img = convert_color(img, color_space=color_space)\n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = cv2.resize(feature_img, size).ravel()\n",
    "    # Return the feature vector\n",
    "    return features\n",
    "\n",
    "# View a random car's spatial bins.\n",
    "plt.figure(figsize=(15, 15))\n",
    "ind = np.random.randint(0, len(cars))\n",
    "img = plt.imread(cars[ind])\n",
    "plt.subplot(121).set_title(\"Original\")\n",
    "plt.imshow(img)\n",
    "feat = bin_spatial(img, size=(SPATIAL, SPATIAL))\n",
    "plt.subplot(122).set_title(\"Resized\")\n",
    "# Convert features back into RGB color space.\n",
    "plt.imshow(cv2.cvtColor(feat.reshape((SPATIAL, SPATIAL, 3)), cv2.COLOR_HLS2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A resolution of about 10x10 is as about as low as we can get while retaining an image something like a car. To divide evenly into 64x64, let's go with 16x16.\n",
    "\n",
    "Our 3D plots showed that the L and S channels are most relevant for picking out cars, so let's include just these two channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bins lightness and saturation information in a `size` matrix, which is then\n",
    "# flattened into a feature vector.\n",
    "def bin_spatial_ls(img, size=(16, 16)):\n",
    "    feature_img = convert_color(img, color_space='HLS')\n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = cv2.resize(feature_img, size)[:, :, (1, 2)].ravel()\n",
    "    # Return the feature vector\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Histogram of Oriented Gradient features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"Project: Vehicle Detection and Tracking, 20. scikit-image HOG\".\n",
    "\n",
    "from skimage.feature import hog\n",
    "\n",
    "ORIENT = 9\n",
    "PIX_PER_CELL = 8\n",
    "CELL_PER_BLOCK = 2\n",
    "\n",
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features_one_channel(img, orient, pix_per_cell, cell_per_block,\n",
    "                                 vis=False, feature_vec=True, transform_sqrt=True):\n",
    "    if vis == True:\n",
    "        # Use skimage.hog() to get both features and a visualization\n",
    "        return hog(img,\n",
    "                   orientations=orient,\n",
    "                   pixels_per_cell=(pix_per_cell, pix_per_cell), \n",
    "                   cells_per_block=(cell_per_block, cell_per_block), \n",
    "                   visualise=True,\n",
    "                   feature_vector=feature_vec,\n",
    "                   block_norm=\"L2-Hys\",\n",
    "                   transform_sqrt=transform_sqrt)\n",
    "    else:      \n",
    "        # Use skimage.hog() to get features only\n",
    "        return hog(img,\n",
    "                   orientations=orient,\n",
    "                   pixels_per_cell=(pix_per_cell, pix_per_cell), \n",
    "                   cells_per_block=(cell_per_block, cell_per_block), \n",
    "                   visualise=False,\n",
    "                   feature_vector=feature_vec,\n",
    "                   block_norm=\"L2-Hys\",\n",
    "                   transform_sqrt=transform_sqrt)\n",
    "\n",
    "# View a random car's HOG visualization.\n",
    "ind = np.random.randint(0, len(cars))\n",
    "img = plt.imread(cars[ind])\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Get HOG features for each color channel.\n",
    "_, gray_hog_img = get_hog_features_one_channel(gray, ORIENT, PIX_PER_CELL, CELL_PER_BLOCK,\n",
    "                                               vis=True, feature_vec=False)\n",
    "\n",
    "# Display the original image and each color channel's HOG visualization.\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(1, 3, 1).set_title('Example Car Image')\n",
    "plt.imshow(img)\n",
    "plt.subplot(1, 3, 2).set_title('Grayscale Car Image')\n",
    "plt.imshow(gray, cmap='gray')\n",
    "plt.subplot(1, 3, 3).set_title('HOG Visualization')\n",
    "plt.imshow(gray_hog_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the number of features, I try and push the pixels per cell value as high as possible. A value of 12 is about as high as can be used without beginning to impact resolution. To divide evenly into 64, let's use 8.\n",
    "\n",
    "I only care about shape information, so to minimize the number of features while increasing/maintaining contrast, I grayscale the image to reduce it to a single channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract and normalize features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"Project: Vehicle Detection and Tracking, 22. Combine and Normalize Features\" & \"29. HOG Classify.\"\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extracts spatial bin and HOG features from one image.\n",
    "def extract_image_features(img, orient, pix_per_cell, cell_per_block, spatial_size=(16, 16)):    \n",
    "    # Get spatial color features.\n",
    "    spatial_features = bin_spatial_ls(img, size=spatial_size)\n",
    "    # Get HOG features.\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    hog_features = get_hog_features_one_channel(gray, orient, pix_per_cell, cell_per_block,\n",
    "                                    transform_sqrt=True)\n",
    "    # Return complete feature vector.\n",
    "    return np.concatenate((spatial_features, hog_features))\n",
    "\n",
    "# Extracts spatial bin and HOG featuers from a list of images.\n",
    "def extract_features(img_path_list, orient, pix_per_cell, cell_per_block, spatial_size=(16, 16)):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for path in img_path_list:\n",
    "        # Read in each one by one\n",
    "        img = plt.imread(path)\n",
    "        # Add features to feature vector list.\n",
    "        features.append(\n",
    "            extract_image_features(img, orient, pix_per_cell, cell_per_block, spatial_size=spatial_size))\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "\n",
    "\n",
    "car_features = extract_features(cars, ORIENT, PIX_PER_CELL, CELL_PER_BLOCK, spatial_size=(SPATIAL, SPATIAL))\n",
    "notcar_features = extract_features(notcars, ORIENT, PIX_PER_CELL, CELL_PER_BLOCK, spatial_size=(SPATIAL, SPATIAL))\n",
    "\n",
    "# Create an array stack of feature vectors\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "# Fit a per-column scaler.\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X.\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "# Plot an example of raw and scaled features\n",
    "ind = np.random.randint(0, len(cars))\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plt.imshow(mpimg.imread(cars[ind]))\n",
    "plt.title('Original Image')\n",
    "plt.subplot(132)\n",
    "plt.plot(X[ind])\n",
    "plt.title('Raw Features')\n",
    "plt.subplot(133)\n",
    "plt.plot(scaled_X[ind])\n",
    "plt.title('Normalized Features')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build and train an SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"Project: Vehicle Tracking, 28. Color Classify\".\n",
    "\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using spatial binning of:', (SPATIAL, SPATIAL))\n",
    "print('and HOG features with', ORIENT, 'orientation bins,', PIX_PER_CELL, 'pixels per cell,')\n",
    "print('and', CELL_PER_BLOCK, 'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "\n",
    "# Set up grid search.\n",
    "parameters = {'kernel': ['linear', 'rbf'], 'C': [5, 10, 15]}\n",
    "svr = SVC(probability=True)\n",
    "clf = GridSearchCV(svr, parameters)\n",
    "\n",
    "# Check the training time for the SVC\n",
    "t = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2 - t, 2), 'Seconds to train SVC...')\n",
    "\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(clf.score(X_test, y_test), 4))\n",
    "\n",
    "# Report best parameter values (to speed up future training).\n",
    "print(clf.best_params_, 'are the best parameter values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Search for vehicle in an image using a sliding window search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"Project: Vehicle Tracking, 34. Search and Classify\" & \"35. Hog Sub-sampling Window Search\".\n",
    "\n",
    "PROB_THRESH = 0.99\n",
    "\n",
    "# Define a function to draw bounding boxes.\n",
    "# `bboxes` is a list of tuples of bounding box opposing corners.\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy\n",
    "\n",
    "# Define a single function that can extract features using HOG sub-sampling\n",
    "# and also make predictions about where cars are located in an image.\n",
    "#\n",
    "# The function works as follows:\n",
    "#   1) HOG features are computed once for the entire search region of the image.\n",
    "#   2) Then sliding windows of different magnifications are applied over the search region.\n",
    "#      HOG features for each window are found by sub-selecting from the complete set of HOG features.\n",
    "#      Color bin features are also computed for the window.\n",
    "#   3) The features for the window are normalized and passed to a classifier to predict whether,\n",
    "#      with high probability, the patch of image within the window contains a car or not.\n",
    "#   4) If a patch is predicted to contain a car, that window is recorded.\n",
    "#   5) The list of all windows predicted to contain cars is returned.\n",
    "#\n",
    "# `ystart` and `ystop` define a y-axis range to search.\n",
    "# `scale` is the magnification to apply to the image prior to searching.\n",
    "# `clf` is a car/not-car classifier.\n",
    "# `X_scaler` is a feature normalizer.\n",
    "# `orient` is the number of orientation bins for HOG features.\n",
    "# `pix_per_cell` is the number of pixels per cell for HOG features.\n",
    "# `cell_per_block` is the number of cells per block for HOG features.\n",
    "# `spatial_size` is the dimensions of the spatial color bins.\n",
    "def find_cars(img, ystart, ystop, scale, clf, X_scaler, orient, pix_per_cell, cell_per_block, spatial_size):\n",
    "\n",
    "    # Define a color-transformed image and the region over which to run the sliding window search.\n",
    "    ctrans_img = convert_color(img, color_space='RGB')\n",
    "    ctrans_tosearch = ctrans_img[ystart:ystop,:,:]\n",
    "    # Potentially magnify the image.\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "    \n",
    "    # HOG features will use the grayscaled search region.\n",
    "    # Spatial bin features will use the original color-transformed search region.\n",
    "    gray = cv2.cvtColor(ctrans_tosearch, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Define blocks and steps.\n",
    "    nxblocks = (gray.shape[1] // pix_per_cell) - cell_per_block + 1\n",
    "    nyblocks = (gray.shape[0] // pix_per_cell) - cell_per_block + 1 \n",
    "    window = 64  # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    nblocks_per_window = (window // pix_per_cell) - cell_per_block + 1\n",
    "    cells_per_step = 1  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step + 1\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step + 1\n",
    "    \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog = get_hog_features_one_channel(gray, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    \n",
    "    bboxes = []\n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb * cells_per_step\n",
    "            xpos = xb * cells_per_step\n",
    "            \n",
    "            # Extract HOG for this patch.\n",
    "            hog_features = hog[ypos:ypos + nblocks_per_window, xpos:xpos + nblocks_per_window].ravel() \n",
    "\n",
    "            xleft = xpos * pix_per_cell\n",
    "            ytop = ypos * pix_per_cell\n",
    "\n",
    "            # Extract the image patch.\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop + window, xleft:xleft + window, :], (64, 64))\n",
    "            # Get color features for the image patch.\n",
    "            spatial_features = bin_spatial_ls(subimg, size=spatial_size)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            test_features = X_scaler.transform(np.hstack((spatial_features, hog_features)).reshape(1, -1))\n",
    "            test_prediction = clf.predict_proba(test_features)[0, 1]\n",
    "            \n",
    "            # If a car was detected, record its bounding box.\n",
    "            if test_prediction > PROB_THRESH:\n",
    "                xbox_left = np.int(xleft * scale)\n",
    "                ytop_draw = np.int(ytop * scale)\n",
    "                win_draw = np.int(window * scale)\n",
    "                bboxes.append(((xbox_left, ytop_draw + ystart),\n",
    "                              (xbox_left + win_draw, ytop_draw + win_draw + ystart)))\n",
    "                \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YSTART = 380  # No cars in the trees; Udacity students haven't developed a flying car company yet!\n",
    "YSTOP = 700  # Should be YSTART + a multiple of `window` (defined in `find_cars()` above)\n",
    "SCALES = [1, 2., 3.]\n",
    "\n",
    "# Plot vehicle detections for all test images.\n",
    "plt.figure(figsize=(15, 30))\n",
    "images = glob.glob('./test_images/*.jpg')\n",
    "for i, path in enumerate(images):\n",
    "    orig_img = plt.imread(path)\n",
    "    img = orig_img.astype(np.float32)/255  # Required for JPEGs.\n",
    "\n",
    "    # Search for vehicles at all scales.\n",
    "    bboxes = []\n",
    "    for scale in SCALES:\n",
    "        bboxes.extend(find_cars(img, YSTART, YSTOP, scale, clf, X_scaler, ORIENT, PIX_PER_CELL, CELL_PER_BLOCK,\n",
    "                      (SPATIAL, SPATIAL)))\n",
    "\n",
    "    plt.subplot(len(images), 2, 2 * i + 1).set_title('Original ' + path)\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(len(images), 2, 2 * i + 2).set_title('Car Positions')\n",
    "    out_img = draw_boxes(orig_img, bboxes, color=(0, 0, 255), thick=6)\n",
    "    plt.imshow(out_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! There are very few false positive identifications.\n",
    "\n",
    "Depending on the run, the black car may or may not be identified, as it enters the shadow cast by the tree (`./test_images/test5.jpg` and `./test_images/test6.jpg`). Hopefully collecting bounding boxes over multiple frames will make it possible to bridge the gap with this car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combine overlapping windows & eliminate false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"Project: Vehicle Detection, 37. Multiple Detections & False Positives\".\n",
    "\n",
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "# Number of overlapping bounding boxes required to identify a labeled object as a car.\n",
    "HEAT_THRESH = 1  # This will be redefined later, when considering consecutive video frames.\n",
    "\n",
    "# Adds heat to a heatmap.\n",
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "\n",
    "    # Return updated heatmap\n",
    "    return heatmap# Iterate through list of bboxes\n",
    "\n",
    "# Applies a threshold to a heat map by zeroing out pixels below the threshold.\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap\n",
    "\n",
    "# Draw a single bounding box around each vehicle identified in `labels`.\n",
    "# This is done by finding, for each vehicle, all pixels associated with that\n",
    "# vehicle and drawing a bounding box around all its pixels.\n",
    "#\n",
    "# `labels` is the output of the `label()` function, which is a tuple of\n",
    "# labeled pixels and number of labels.\n",
    "#   * The labels identify each detected vehicle\n",
    "#   * The labeled pixels have the same shape as `img`, but each pixel is assigned\n",
    "#     a detected vehicle label.\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1] + 1):\n",
    "        # Find pixels with each car_number label value.\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels.\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y.\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image.\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0, 0, 255), 6)\n",
    "    # Return the image.\n",
    "    return img\n",
    "\n",
    "# Draw a single bounding box around each detected vehicle in `image`,\n",
    "# given a list of all windows identified as containing a vehicle.\n",
    "def draw_bounding_boxes(image, bboxes_list):\n",
    "    heat = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "\n",
    "    # Add heat to each box in box list\n",
    "    heat = add_heat(heat, bboxes_list)\n",
    "    \n",
    "    # Apply threshold to help remove false positives\n",
    "    heat = apply_threshold(heat, HEAT_THRESH)\n",
    "\n",
    "    # Visualize the heatmap when displaying    \n",
    "    heatmap = np.clip(heat, 0, 255)\n",
    "\n",
    "    # Find final boxes from heatmap using label function\n",
    "    labels = label(heatmap)\n",
    "    draw_img = draw_labeled_bboxes(np.copy(image), labels)\n",
    "    return heatmap, draw_img\n",
    "\n",
    "# Plot a single vehicle detection bounding box per detected vehicle and\n",
    "# the pre-thresholding heat maps for all test images.\n",
    "plt.figure(figsize=(15, 20))\n",
    "images = glob.glob('./test_images/*.jpg')\n",
    "for i, path in enumerate(images):\n",
    "    orig_img = plt.imread(path)\n",
    "    img = orig_img.astype(np.float32)/255  # Required for JPEGs.\n",
    "\n",
    "    # Search for vehicles at all scales.\n",
    "    bboxes = []\n",
    "    for scale in SCALES:\n",
    "        bboxes.extend(find_cars(img, YSTART, YSTOP, scale, clf, X_scaler, ORIENT, PIX_PER_CELL, CELL_PER_BLOCK,\n",
    "                      (SPATIAL, SPATIAL)))\n",
    "\n",
    "    plt.subplot(len(images), 3, 3 * i + 1).set_title('Original ' + path)\n",
    "    plt.imshow(img)\n",
    "    heatmap, out_img = draw_bounding_boxes(orig_img, bboxes)\n",
    "    plt.subplot(len(images), 3, 3 * i + 2).set_title('Car Positions')\n",
    "    plt.imshow(out_img)\n",
    "    plt.subplot(len(images), 3, 3 * i + 3).set_title('Heat Map')\n",
    "    plt.imshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying thresholding to the heat map of overlapping bounding boxes is able to eliminate the false positive and draw tight boxes around the cars in most cases (`./test_images/test5.jpg` as the while car recedes out of the image is an exception)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate final video output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from CarND-LaneLines-P1/P1.ipynb\".\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "HEAT_THRESH = 10\n",
    "FRAME_WINDOW = 20  # The number of frames to look back.\n",
    "prev_bboxes = []  # Vehicle detections for all frames.\n",
    "\n",
    "def process_image(get_frame, t):\n",
    "    # NOTE: The output you return should be a color image (3 channel) for processing video below\n",
    "    orig_img = get_frame(t)\n",
    "    img = orig_img.astype(np.float32)/255  # Model was trained on PNGs, frames are JPEGs.\n",
    " \n",
    "    # Search for vehicles at all scales.\n",
    "    bboxes = []\n",
    "    for scale in SCALES:\n",
    "        bboxes.extend(find_cars(img, YSTART, YSTOP, scale, clf, X_scaler, ORIENT, PIX_PER_CELL, CELL_PER_BLOCK,\n",
    "                                (SPATIAL, SPATIAL)))\n",
    "    # Record raw vehicle detections.\n",
    "    prev_bboxes.append(bboxes)\n",
    "    \n",
    "    # Prior to drawing tight bounding boxes, concatenate the lists of bounding boxes\n",
    "    # found in the last FRAME_WINDOW frames.\n",
    "    flat = [bbox for bboxes in prev_bboxes[-FRAME_WINDOW:] for bbox in bboxes]\n",
    "    _, draw_img = draw_bounding_boxes(orig_img, flat)\n",
    "    return draw_img\n",
    "\n",
    "output = './output_images/project_video.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "clip = VideoFileClip('./project_video.mp4')\n",
    "output_clip = clip.fl(process_image)\n",
    "%time output_clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
